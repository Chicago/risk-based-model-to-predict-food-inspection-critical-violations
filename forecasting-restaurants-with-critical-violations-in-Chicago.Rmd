---
title: "Forecasting restaurants with critical violations in Chicago"
author: Tom Schenk Jr. (City of Chicago), Gene Leynes (City of Chicago), and Aakash
  Solanki (City of Chicago), Stephen Collins (Allstate Insurance), Gavin Smart (Allstate
  Insurance), Ben Albright, and David Crippin (Allstate Insurance)
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    css: ../assets/journal-chicago/css/journal-chicago.css
    number_sections: yes
  pdf_document:
    number_sections: yes
  word_document: default
bibliography: references.bib
---


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
##==============================================================================
## INITIALIZE
##==============================================================================
# if(interactive()){
#     ## Remove all objects; perform garbage collection
#     rm(list=ls())
#     gc(reset=TRUE)
#     geneorama::detach_nonstandard_packages()
# }
geneorama::loadinstall_libraries(c("data.table", "glmnet", "ggplot2", "knitr",
                                   "splines", "reportr", "scales", "knitcitations"))

## Define melt to be the data.table melt
melt <- data.table:::melt.data.table

```

```{r, echo=FALSE, results='hide', fig.show='hide', message=FALSE, warning=FALSE}
##==============================================================================
## RUN GLMNET MODEL
##==============================================================================

## NOTE: THE DIRECTORY NAME BELOW MUST MATCH YOUR PROECT DIRECTORY NAME
## If you clone the project directly from github.com the project name will
## automatically be food-inspection-evaluation, but if you change the name 
## you will need to update the name of the project below in the set_project_dir
## function below.
geneorama::set_project_dir("food-inspections-evaluation")
geneorama::sourceDir("CODE/functions/")

## Reads-in BibTex generated by Zotero library "Food Inspections"
biblio <- read.bibtex("REPORTS/forecasting-restaurants-with-critical-violations-in-Chicago.bib")
options("citation_format" = "pandoc")

## Overwrite the base "interactive" function to prevent complete initialization
interactive <- function(){FALSE}
## Source the file that runs the glmnet model:
source("CODE/30_glmnet_model.R", echo = FALSE)
## Remove the temporary local "interactive" function
rm(interactive)
## Remove excess variables generated by "CODE/30_glmnet_model.R"
rm(confusion_values_test, dat, errors, errorsTest, iiTest, iiTrain, 
   lam, mm, pen, w.lam, xmat)

## These should be the only remaining objects (besides functions) imported:
# coef
# inspCoef
# datTest
# model

## List objects (besides functions) to check with comments above:
geneorama::lll()

## Load Food Inspection Data
food <- readRDS("DATA/food_inspections.Rds")
```


```{r, echo=FALSE}
## Set global knitr option to NOT echo code
opts_chunk$set(echo = FALSE)
```


```{r, results='hide'}
##==============================================================================
## DEFINE CUSTOM GGPLOT FUNCTION
##==============================================================================

# cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", 
#                "#0072B2", "#D55E00", "#CC79A7")

ggplot <- function(...) {
    cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", 
               "#0072B2", "#D55E00", "#CC79A7")
    ggplot2::ggplot(...) +
        theme_grey()+
        scale_fill_manual(values=cbPalette) +
        scale_colour_manual(values=cbPalette)
        # + theme(plot.title = element_text(size = 20))
    
}
```

```{r, results='hide'}
# cbPalette <- c("#000000", "#E69F00", "#56B4E9", 
#                "#009E73", "#F0E442", "#0072B2", 
#                "#D55E00", "#CC79A7")
# pie(rep(1,length(cbPalette)), col = cbPalette)
```

```{r, results='hide'}
comp <- cbind(
    datTest[i = order(Inspection_Date), 
            j = list(Inspection_ID_BAU   = Inspection_ID,
                     Inspection_Date   = Inspection_Date,
                     criticalFound_BAU     = criticalFound)],
    datTest[i = order(-glm_pred), 
            j = list(Inspection_ID_Model   = Inspection_ID,
                     criticalFound_Model   = criticalFound)])
comp <- comp[ , Best_Possible := criticalFound_Model[order(-criticalFound_Model)]]
comp <- comp[ , Worst_Possible := criticalFound_Model[order(criticalFound_Model)]]

comp_summary <- comp[
    i = TRUE, 
    j = list(Total_Inspections = .N,
             Crit_Violations_BAU = sum(criticalFound_BAU),
             Crit_Violations_Model = sum(criticalFound_Model),
             Best_Possible = sum(Best_Possible),
             Worst_Possible = sum(Worst_Possible)), 
    keyby = list(Inspection_Date = Inspection_Date)]
```

```{r prepare_calc_for_difference}
date_comp <- merge(
    x = comp[i = TRUE,
             j = list (criticalFound_BAU,
                       date_bau = Inspection_Date),
             keyby = list(id = Inspection_ID_BAU)],
    y = comp[i = TRUE,
             j = list (criticalFound_Model,
                       date_model = Inspection_Date),
             keyby = list(id = Inspection_ID_Model)],
    by = "id")

# Store this data as a variable to be used in text
time_diff <- date_comp[criticalFound_Model==1, -(date_model - date_bau)]
```
```{r calculate_violation_rates}
# datTest[period==1, list(.N, sum(criticalFound))]
# datTest[period_modeled==1, list(.N, sum(criticalFound))]

crit_viol_rate <- data.table(
    Regime = c("Business\nAs Usual", 
               "Data\nDriven"),
    values = c(datTest[period==1, sum(criticalFound)/.N],
               datTest[period_modeled==1, sum(criticalFound)/.N]))
crit_viol_cumulative <- data.table(
    Regime = c("Business\nAs Usual", 
               "Data\nDriven"),
    values = c(datTest[period==1, sum(criticalFound)] / datTest[ , sum(criticalFound)],
               datTest[period_modeled==1, sum(criticalFound)] / datTest[ , sum(criticalFound)]))
```

```{r, results='hide'}
## These are the calculations to support the rounded numbers in the introduction

## Total inspections by Risk
food[year(Inspection_Date)==2014 & grepl("Risk", Risk), .N, keyby=Risk]
food[year(Inspection_Date)==2014 & grepl("Risk", Risk), .N, keyby=Risk][,sum(N)]
## Licenses inspected by Risk
food[year(Inspection_Date)==2014 & grepl("Risk", Risk), .N,list(License, Risk)][
    i=TRUE, .N, keyby=Risk]
food[year(Inspection_Date)==2014 & grepl("Risk", Risk), .N,list(License, Risk)][
    i=TRUE, .N, keyby=Risk][,sum(N)]
```

```{r}
## Tables used in discussion of inspection types

geneorama::set_project_dir("food-inspections-evaluation")
source("CODE/functions/categorize.R")
# food[year(Inspection_Date)==2014,.N,Inspection_Type][order(-N)]
insp_types_2014 <- food[year(Inspection_Date)==2014, list(Inspection_Type)]
# insp_types_2014
insp_types_primary <- c("Canvass", "Canvass Re-Inspection", 
                           "License", "License Re-Inspection",
                           "Complaint", "Short Form Complaint",
                           "Complaint Re-Inspection")
insp_types_2014 <- insp_types_2014[
    i = TRUE , 
    j = Inspection_Type := categorize(x = Inspection_Type, 
                                      primary = insp_types_primary)]
```


```{r}
## Tables used in discussion of risk types

risk_2014 <- food[year(Inspection_Date)==2014 & grepl("Risk", Risk),
     .N,list(License, Risk)]
risk_2014_table <- rbind(risk_2014[,.N,keyby=Risk], 
                         data.table(Risk = "TOTAL", 
                                    N = nrow(risk_2014)))
risk_2014_table_formatted <- risk_2014_table[
    i = TRUE, 
    j = list(`Risk Category` = Risk, 
             `Count of Licenses` = geneorama::comma(N))]

```

> The Chicago Department of Public Health (CDPH) inspects more than 15,000 restaurants with fewer than three dozen inspectors over the course of the year. This paper describes a predictive model designed to identify the presence of a critical violation in a particular food establishment. The goal of this model is to prioritize inspections by likelihood in order to identify the riskiest restaurants earlier, thereby reducing the length of exposure of risky restaurants to patrons. Critical violations were identified approximately `r sprintf("%0.2f", mean(time_diff))` days earlier over a 60 day period compared to current operations in the out-of-sample test.

# Introduction

In 2014 the [Chicago Department of Public Health](http://www.cityofchicago.org/city/en/depts/cdph.html) inspected performed over 20,000 inspections at nearly 13,000 food establishments across Chicago with fewer than three dozen inspectors. The majority of these food inspections were routine inspections that don't uncover serious problems, but some of these inspections uncovered issues that affect the health and safety of the patrons who visit these establishments. Traditionally, prioritizing these inspections is a largely manual task that relies on a combination of administrative processes and personal expertise. 

The model set forth in this paper can help with the prioritization of scheduled, saving time and money as well as making the city's food safer. The model utilizes several data sources and through advanced modeling techniques the model provides additional insight into an establishment's current actual risk based on real-time data. 

This paper is organized as follows: Section 1 provides an introduction and background to describe the current process and scope of the problem. Section 2 describes data that has been collected by the research team for this project and how that data was combined. Section 3 describes the model.  Section 4 describes the model evaluation. Section 5 contains details of the model results from the experiment.  Finally, the Summary section concludes with a brief summary of results and information regarding the ongoing project.

Ultimately, we find that a data-driven model can help inspectors discover critical violations earlier than the current "Business As Usual" (BAU) process. On average, critical violations would have been discovered `r sprintf("%0.2f", mean(time_diff))` days earlier over the two-month test period. The first half of the experiment yielded `r paste0(sprintf("%0.1f", crit_viol_rate[,diff(values)/values[1]]*100), "%")` higher successful inspections. Beginning in 2015, CDPH has begun to use this analytical model to prioritize canvas inspections. Risk 1 and risk 2 Food Establishments will still undergo annual inspections; however, these restaurants with the highest likelihood of the most serious issues will be prioritized.

It is worth nothing that this research is an open source project. The source code of the statistical model is available on the City of Chicago [food inspection project page](http://github.com/Chicago/food-inspections-evaluation). The statistical modeling was completed using the open source statistical software R, and all the necessary data to replicate these results is available online. This paper was generated using [knitr](http://yihui.name/knitr), which allows others to view the underlying calculations to generate the summaries, tables, and diagrams in this document. This document is available in the same aforementioned repository.

CDPH mainly conducts three different types of inspections; regular canvass inspections, new license inspections, and inspections in response to a complaint. Currently regular canvas visits are the only type of inspection included in the model.

Before a food establishment opens their doors CDPH conducts an initial new license inspection. New license inspections are coordinated with [Business Affairs and Consumer Protection (BACP)](http://www.cityofchicago.org/city/en/depts/bacp.html), who grants food establishment licenses to new establishments. Each establishment must pass this initial inspection before it is allowed to serve food to patrons. Establishments often fail these initial inspections because they have not yet finished setting-up equipment, such as turning-on a refrigerator, or have not finished construction. Under these circumstances, CDPH will re-inspect those establishments to ensure those conditions are passed before they are allowed to open.  New license inspections are not used in the model because we believe that they are not characteristic of normal inspections, and because they occur when a new business applies for a license, and are therefore not schedule. 

The majority of the food inspections are regular canvass visits, which must be done on a regular basis to check the quality of sanitary conditions. The frequency of canvass inspections is driven by the risk level of the facility. Risk 1 establishments are ideally inspected two times a year.

If a restaurant fails the inspection, because of violations / citations, the inspector will return at a later date to see if the situation has been remediated. These reinspections have a very high pass rate, and are not included in the model. Only the initial canvass inspection is included.

The third type of inspection occurs when complaints are registered from residents, alderman, and referrals from hospitals. Often, these requests are driven through the City of Chicago's 311 system, which can be submitted through residents calling 311 or submitting a request through an online form. Individuals are asked to submit where they believe they contracted food poisoning, the address of the establishment, describe the symptoms and what was eaten, and when it happened. CDPH reviews the materials and may initiate a food inspection if it does seem the illness and restaurant can be linked together.

A breakdown of the inspection types in 2014:

```{r}
kable(insp_types_2014[i = TRUE,
                      list(`2014 Count` = geneorama::comma(.N)), 
                      keyby = Inspection_Type][c(insp_types_primary, "Other")], 
      align = c("l", "r"))
```

Uniquely, CDPH also encourages submissions through the [Foodborne Chicago](http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6332a1.htm) program. Foodborne's [Machine learning algorithms](https://github.com/smartchicago/foodborne) scan Twitter looking for individuals complaining or indicating potential food poisoning cases. These tweets are identified and a human will contact the user, providing a link and information on how they can [report their complaint](https://www.foodbornechicago.org/) to CDPH. In a nine-month span, 133 food inspections were instigated from this program, where 20 percent (27 instances) of those inspections resulted in critical violations `r citep(biblio[[2]])`.

The Foodborne program and 311 system has assisted CDPH in targeting and identifying complaint-driven requests. Yet, a sizeable task to complete regular canvas inspections remains. Canvas inspections occur throughout the year and are somewhat random inspections of various restaurants. The process is key to checking and enforcing consistent food safety practices throughout the city. Identifying critical issues at restaurants help rectify those issues and reduce exposure to patrons. 

The risk levels are determined by food handling practices required for each establishment. Restaurants and other establishments are generally categorized as risk 1 if they directly handle ingredients, prepare food, or if they cool or heat food. The lowest risk establishments generally consist of prepackaged and non-perishable food. 

The Risk level also drives frequency of inspection. Risk 1 facilities are inspected more frequently, with a target of two annual inspections; risk 2 establishments are inspected at least once a year; and risk 3 establishments are inspected once every other year.

Although Risk levels help prioritize inspections by focusing on higher risk establishments, in 2014 [`r percent(risk_2014[Risk=="Risk 1 (High)",.N] / risk_2014[,.N])`] of the food establishment licenses were categorized as risk 1. The high proportion of risk 1 establishments means there is still a substantial queue to be inspected.

```{r, results='hide'}
## Originally this calculation was used for the "number of inspections per day"
## However, the risk variable is no longer in the data (since we're not using
## it in the model)
## Also, "dat" from the 30 file represented all the records that had no NA's
## over the past several years, so it would be over counting each risk class
## because it's not grouped by License
# format(round(table(dat$Risk)[3]/32/230, 0), nsmall=0)

## This is closer to the desired calculation, but the result doesn't seem right
## (I think we have fewer than 32 full time inspectors)
format(round(risk_2014_table[Risk=="Risk 1 (High)", N]/32/230, 0), nsmall=0)

## Original paragraph:
## Although Risk levels help prioritize inspections by focusing on higher risk establishments, in 2014 [`r percent(risk_2014[Risk=="Risk 1 (High)",.N] / risk_2014[,.N])`] of the food establishment licenses were categorized as risk 1. The high proportion of risk 1 establishments means there is still a substantial queue to be inspected. Yet, the work is certainly achievable. Assuming 32 inspectors, each inspector would need to complete [`r format(round(table(dat$)[3]/32/230, 0), nsmall=0)`] canvass inspections each working day---in addition to complaint-driven and new license inspections.
```

A summary of the risk types appears below:

```{r, fig.width=4, fig.show='hold'}
kable(risk_2014_table_formatted, align = c("l", "r"))

## Can't make plots and tables appear side by side!!! 
## so omitting this plot

# ggplot(risk_2014) + 
#     aes(x = Risk) + 
#     labs(title=paste0('Breakdown of Food Establishments\n',
#                       'by Risk Classification\n') ) +
#     geom_bar() + 
#     # guides(fill=FALSE) +
#     # scale_y_continuous(labels = percent) +
#     xlab("") + ylab("") + theme(legend.position="none")
```

# Data

Thanks to a long-standing progressive stance on information technology and data collection, the City of Chicago has world class data in terms of quality, availability, and accessibility. Data availability was a key factor in the ability to build this model. Multiple sources of data and variables were tested and used in the development of the model, including information about previous food inspections, information about business licenses, information about associated business licenses, and events around each food establishment, such as 311 complaints, crime, and weather.

The City of Chicago publishes over 600 datasets on the [open data portal](https://data.cityofchicago.org), including the results of food inspections from 2011 to present. The [food inspection dataset](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5) includes the name of the establishment, address, risk level, inspection date, results, and a detailed list of violations found during the inspection.

At this time, the food inspection database is not maintained and hosted by the City of Chicago, instead, a file of all food inspections are sent to the City of Chicago open data team on a daily basis, which is automatically uploaded to the online data portal every morning. Thus, the rawest form of data available to the research team was the same data available on the open data portal.

In addition, the City also publishes other relevant data on the portal, including: business licenses from 2011 to present, detailed crime data from 2011 to present, and various 311 data, including garbage and sanitation complaints. All of the data that was used in the model came from the [data portal](https://data.cityofchicago.org), except the weather data and sanitarian data.

## Historical data

For modeling purposes the food inspection history formed the basis of our analysis. Each regular canvass inspection was used as an observation in the model. Some filtering rules were applied. We filtered the inspections to only include Retail Food Establishments, which excluded establishments such as schools and hospitals. Their inspection schedules follow a different planning process, and we also believe that these establishments have different risk characteristics that are not generalizable across the entire population. We also excluded inspection records that didn't result in an inspection because the business was closed.

Each historical inspection record contained an "Inspection Key" that made it possible to uniquely identify and group inspections. Also, each record contained a license number which could later be tied back to the original business license.

Several model variables were created based on the food inspection history, including: time since last inspection, if this inspection was their first canvass inspection, and (not _currently_ used) the facility type.

There are 42 different possible violations that can be cited by CDPH. Often, these violations are classified into three categories: critical, serious, and minor violations. Critical violations consist of 14 different violations that are most likely to create conditions for food born illnesses, such as failure to heat food to proper temperatures or to keep items properly refrigerated at the proper temperatures. Conversely, minor violations can be as simple as leaving a rag in the sink. Restaurants can fail their inspections with as little as one critical violation, however serious and minor violations during an inspection can also culminate to a failed outcome.

We included the results of the previous inspection in the model, which was one of the most important variables. We used previous serious and critical violations in the model, looking back one period, but we did not use minor violations as an indicator.

Food inspection history is combined with business license data published by BACP. Any food seller must not only be licensed by the City but also obtain licenses for other activities, such as cigarette sales and liquor licenses before they can start selling such items. The license data provides other information about the business, including when certain licenses were first obtained--an approximation for the age of business which allows for calculating age of the food establishment at inspection. 

Food inspection and Business license datasets are matched based on business names and business addresses. License description information found in the BACP data is also added to the analysis-ready data set. It lists whether a food establishment is a retail food establishment or a tobaco retail over the counter. The BACP data also consists of information whether the license is active or not, when was the application for obtaining a license to do business was filed among other information. 

The BACP dataset is important in the sense that in combination with the Food Inspection dataset it curates a full list of all restaurants in the city on which predictions can be made. 

The location of the businesses are used to calculate nearby activity. Several variables were explored, but after conduting some data mining, we settled on burglaries, sanitation code complaints, and garbage cart requests. The density of each activity was calculated and stored.

Weather data was obtained from [forecast.io](www.forecast.io). The data contains a significant wealth of information on not only highs, lows, and percipitation, but also on granular weather forecasts for any latitude and longitude. After several conversations with CDPH staff, we focused on the relationship of temperatures and inspections. High temperatures can lead to issues to cooling food within a food establishment, which results in a critical violation. Some empirical testing of that hypothesis helped support its inclusion.

# Model Development

The final variables used in the model are displayed below:

```{r, eval=FALSE}
## Print the names of the coefficients / copy to clipboard
rownames(coefficients(model))
# geneorama::clipper(rownames(coefficients(model)))
```

Variable Name (Literal)                       | Variable Description
----------------------------------------------|---------------------------------
`Inspectorblue`                               | Indicator variable for Sanitarian Cluster 1
`Inspectorbrown`                              | Indicator variable for Sanitarian Cluster 2
`Inspectorgreen`                              | Indicator variable for Sanitarian Cluster 3
`Inspectororange`                             | Indicator variable for Sanitarian Cluster 4
`Inspectorpurple`                             | Indicator variable for Sanitarian Cluster 5
`Inspectoryellow`                             | Indicator variable for Sanitarian Cluster 6
`pastCritical`                                | Indicates any previous critical violations (last visit)
`pastSerious`                                 | Indicates any previous serious violations (last visit)
`timeSinceLast`                               | Elapsed time since previous inspection
`ageAtInspection`                             | Age of business license at the time of inspection
`consumption_on_premises_incidental_activity` | Presence of a license for consumption / incidental activity
`tobacco_retail_over_counter`                 | Presence of an additional license for tobacco sales
`temperatureMax`                              | The daily high temperature on the day of inspection
`heat_burglary`                               | Local intensity of recent burglaries
`heat_sanitation`                             | Local intensity of recent sanitation complaints
`heat_garbage`                                | Local intensity of recent garbage cart requests



The principle question is whether we can reasonably determine the probability that a restaurant inspection will yield at least one critical violation. That is, the focus will be whether or not any critical violation is found---a binary response. We use a glmnet model to estimate the impact.

While the following form can be expressed a number of ways, the logistic form is commonly expressed as the "log-odds transformation". 

$$
\begin{aligned}
\log = \frac{\text{Pr}(V=1|X=x)}{\text{Pr}(V=0|X=x)} = \beta_0 + \beta^T x
\end{aligned}
$$

Thus, the objective function is to minimize 

$$
\begin{aligned}
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} -\left[\frac{1}{N} \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \beta) - \log (1+e^{(\beta_0+x_i^T \beta)})\right] + \lambda \big[ (1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1\big]
\end{aligned}
$$

A review of the methods to find this solution is provided by `r citep(citation(package = "glmnet"))`, whose glmnet library for R was used to provide estimates. The `r citep(citation(package = "MASS"))` was also used in the analysis.

## Significant Variables 

Several variables were found to be significant when building the model. Many variables were tested, some were discarded, and as time goes on many may find their way back into the model. 

The past performance of food inspections was one of the leading indicators of current likelihood for a critical violation. Both critical and serious violations from the previous inspection were used as a predictor of future performance. In effect, past performance predicted future outcomes, with those with critical violations more likely to repeat those violations than even those with, at most, serious violations.

The elapsed time since the last violation was also a significant variable. The longer that it had been since an inspection the more likely the sanitarian was to find at least one critical violation. However, restaurants' scores decreased over the lifespan of the restaurant. As restaurants grow older, they are less likely to have critical violations while long time-periods between inspections increased the likelihood.

Environmental characteristics were also indicators of future performance. Trends in weather, nearby reports of burglary, and complaints about sanitation and garbage are all significant variables in the model. An increase in the moving three-day average high temperature was associated with more critical violations. In conversations with inspection managers, researchers understood this to be associated with potential mechanical failures---driven by the heat---of equipment that maintained food temperature, a main source of critical violations.

Sanitation code complaints are one of the top complaints registered with the City of Chicago through its 311 system (including web and text reports). Sanitation code complaints include several types of complaints such as overflowing garbage cans, food left outside, or litter.

The largest source of influence in the model was which sanitarian performed the inspection. We included the effect of the individual sanitarian, but we also anonymized the data to protect the individual identity of the sanitarian. Initial results with individual sanitarian coefficients were used to group the sanitarians into clusters. Those clusters were  then arbitrarily assigned color code names.  This masking had very little effect on the model performance, but makes it very difficult to tell which sanitarian performed which inspection.

A summary of the model coefficient is presented below:

```{r table_of_coefficients}
# coef_df <- data.frame(Variables = names(coef),
#                       Coefficients = coef)
knitr::kable(data.table(Variables = names(coef),
                        Coefficients = coef), 
             digits=3, 
             caption="Table of Coefficients")
# kable(as.data.frame(coef), format="html", caption="hi")
# kable(head(iris), format = "html", caption = "Title of the table")

```

# Evaluation

After developing the statistical model to predict critical violations, the research team evaluates whether the model could optimize food inspection processes. Namely, the model is used to determine how much faster the food inspection team can discover critical violations. The team uses a simulation to compare real-life results to an alternate, data-driven arrangement.

After formulating the analytical model, the the principal question for researchers turned to whether this analytical model provides more efficiency for the food inspection team. CDPH operational procedures requires the department to inspect every risk 1 and risk 2 restaurant. Therefore, the operational goal is to allow inspectors to discover critical violations earlier than their current operations (business-as-usual).

One approach for an evaluation may have also sought to determine if the predictive model could discover more restaurants with critical violations. Since CDPH is required to inspect every risk 1 and risk 2 restaurant, discovering more restaurants is not a pertinent goal. Instead, it serves a greater public interest to discover violations sooner, thereby, reducing the potential exposure of conditions that breed foodborne illnesses to the public.

## Evaluation Design

The analytical model was trained on data from January 2011 through January 2014. The researchers waited until CDPH completed food inspections in September and October 2014. This timeframe ensured significant time passed between the test period (January 2011 through January 2014) and the evaluation period to reduce any incidental correlation between the two periods. CDPH was not aware this timeframe would be used for an evaluation in order to prevent against a Hawthorne Effect or other bias. Again, to reduce any potential to bias within reason, senior management at CDPH was aware of on-going research, but sanitarians were not informed of the research. Finally, several months passed between model development and the evaluation period, reducing a perception of the evaluation period.

The evaluation period lasted two months, from `r datTest[ , as.POSIXct(min(Inspection_Date))]` to `r datTest[ , as.POSIXct(max(Inspection_Date))]` and calculate the percentage of inspections that result in critical violations in the first half of the inspections during this period. The number of violations found during this period can be considered as status quo or current mode of operation. It serves as a baseline to capture performance levels of sanitarians, namely, the proportion and rate of critical violations that are found.

Meanwhile, we calculate the point predictions for each establishment using the training data from 2011 through 2014. The training data does not include the evaluation period so not to provide additional feedback from the evaluation period. We sort the establishments that were inspected during the evaluation in descending order of predicted values, placing the highest risk restaurants at the top of the list.

We calculate the percentage of those restaurants that would be inspected in the first half if the predictive model was used. The difference between the percentage of establishments found with critical violations during this period reflects the relative gain or loss of efficiency. Finding a greater percentage of critical violations with the predictive model indicates results can be found earlier. A similar or reduced amount indicates the predictive model provides no benefit or is less efficient, respectively.

Note that this experimental design is assumed to yield the name number of restaurants found with critical violations. Indeed, under the premise that CDPH will inspect all restaurants, researchers will presume the number of violations will remain relatively the same. The objective of the model is to find critical violations earlier throughout the year.

# Results

CDPH completed `r geneorama::comma(datTest[, .N])` inspections between `r datTest[ , as.POSIXct(min(Inspection_Date))]` and `r datTest[ , as.POSIXct(max(Inspection_Date))]`. During this time, CDPH found `r datTest[ , sum(criticalFound)]` violations, `r datTest[ , paste0(round(100*(sum(criticalFound)/.N), 1), "%")]`percent of all inspections. The rate of violations is consistent with the historical average of approximately 15 percent. While the rate of violations is slightly higher, it is close enough where we do not suspect this period is abnormal, thus, a valid comparison for our evaluation.

```{r bar_graph_comparing_BAU_and_model_in_first_half, fig.show='hold', fig.width=3}
ggplot(crit_viol_rate) + 
    aes(x = Regime, y = values) + 
    labs(title=paste0('Percentage of inspections\n',
                      'resulting in a critical violation\n',
                      'during Period 1\n') ) +
    geom_bar(stat = "identity") + 
    # guides(fill=FALSE) +
    scale_y_continuous(labels = percent) +
    xlab("") + ylab("") + theme(legend.position="none")

ggplot(crit_viol_cumulative) + 
    aes(x = factor(Regime), y = values) + 
    labs(title=paste0('Percentage of period 1 & 2\n',
                      'critical violations\n',
                      'found in Period 1\n') ) +
    geom_bar(stat = "identity") +
    # guides(fill=FALSE) +
    scale_y_continuous(labels = percent) +
    xlab("") + ylab("") + expand_limits(y = 1) + theme(legend.position="none")
```

On average, food establishments were identified `r sprintf("%0.2f", mean(time_diff))` days earlier under a data-driven model. Generally, critical violations would have been found sooner under the data-driven regime. The rate of finding critical violations in the first half of the would have increased by `r paste0(sprintf("%0.1f", crit_viol_rate[,diff(values)/values[1]]*100), "%")` under the data-driven model. `r paste0(sprintf("%0.1f", crit_viol_cumulative[Regime=='Business As Usual',values]*100), "%")` of critical food violations were found in the first half; meanwhile, under the data-driven model, `r paste0(sprintf("%0.1f", crit_viol_cumulative[Regime=='Data Driven',values]*100), "%")` of all of the critical violations (an increase of `r paste0(sprintf("%0.1f", crit_viol_cumulative[,diff(values)/values[1]]*100), "%")`). 

While the average gain was `r sprintf("%0.0f", mean(time_diff))` days, there was a significant range in the change. Some restaurants were identified `r max(time_diff)` days earlier than business-as-usual. Half of the crticial violations were identified over `r quantile(time_diff)[3]` days earlier while quarter of all violations were prioritized over `r quantile(time_diff)[4]` days sooner. Yet, some restaurants would be prioritized lower, `r sum(time_diff < 0)` restaurants were incorrectly prioritized lower and were found to have critical violations later--`r paste0(sprintf("%0.0f", sum(time_diff < 0) / length(time_diff)*100), "%")` of the observed critical violations.

```{r graph_of_time_diff, fig.width=6.5, fig.height=5}
hist(as.numeric(time_diff),
#      main = paste0("Distribution of the difference \n",
#                    "in time to discover a critical violation"),
     main = paste0("During the test the data diriven approach would have \n",
                   "generally found critical violations sooner"),
     breaks = 30, 
     xlim = c(-60, 60),
     xlab = "Number of days earlier / (later) that a \ncritical violation would have been discovered",
     col = c(rep("#CC79A7", 10), rep("#009E73", 20)))
text(40, 20, "Green = Improved \n                    performance")
text(-45, 20, "Magenta = Reduced \n                        performance")
```
```{r time_diff_t-test, warning=FALSE, results='hide', message=FALSE}
time_diff_t_test <- t.test(time_diff, mu=0)
```
We conducted a t-test to measure whether the reduction in time to find a critical violation was greater than zero. Namely, the null hypothesis is the average time each food inspection was accelerated is equal to zero. The test ($\sigma =$ `r sprintf("%0.2f", sd(time_diff))`, df = `r time_diff_t_test$parameter`) resulted in a p-value of `r sprintf("%.3e", time_diff_t_test$p.value)`, which indicates that the model is extremely likely to be significant.

Below, Gini curves show the relative difference in the inspection regimes throughout the pilot. Since the first day, the data-driven model revealed more critical violations. Specifically, [111] more violations were found in the first week between September 2 and September 5 ([141] under data-driven compared to [30] for business-as-usual). The cumulative number of violations found were always higher for the data-driven approach until the final day of the pilot.

```{r}
## Create data for cumulative violation summaries
comp_summary_cumsum <- comp_summary[
    i = TRUE,
    j = list(`\nInspection Date` = Inspection_Date,
             `Business As Usual` = cumsum(Crit_Violations_BAU),
             `Data Driven` = cumsum(Crit_Violations_Model),
             `Best Possible` = cumsum(Best_Possible),
             `Worst Possible` = cumsum(Worst_Possible))]
comp_summary_cumsum_subset <- comp_summary_cumsum[
    i = TRUE, 
    j = list(`\nInspection Date`,
             `Business As Usual`,
             `Data Driven`)]
```

```{r, fig.width=10, fig.height=8}
ggplot(melt(data = comp_summary_cumsum_subset, 
            id.vars = "\nInspection Date")) +
    aes(x = `\nInspection Date`, 
        y = value, 
        colour = variable) + 
    labs(title="Comparing cumulative violations discovered\n") +
    ylab("Cumulative critical violations to date\n") +
    geom_line(lwd=1.5) + 
    geom_point(colour="black") +
    theme(legend.position="bottom")
```

By extension, the rate of finding critical violations is higher for the initial quarter of the pilot. The rate of the violations are higher in the first portion as the analytic model correctly ranks higher-risk restaurants for earlier inspection. Business-as-usual has a more consistent rate of discovery, approximately [0.2] per day and stays above [0.1] violations per day. However, whereas the data-driven model is more successful early, the rate of finding violations declines in the last quarter of the pilot.
```{r, fig.width=10, fig.height=8}
ggplot(
    melt(data = comp_summary[
        i = TRUE,
        j = list(`Business As Usual` = 
                     Crit_Violations_BAU / Total_Inspections,
                 `Data Driven` = 
                     Crit_Violations_Model / Total_Inspections),
        keyby = list(`\nInspection Date` = Inspection_Date)], 
            id.vars = "\nInspection Date")) +
    aes(x=`\nInspection Date`, y=value, colour=variable, fill=variable) + 
    labs(title=paste0('Critical violations found on a daily basis\n',
                      'as a percent of total daily inspections\n',
                      "(smoothed results)\n") ) +
     ylab("Percent of inspections resulting \n in critical violations\n") +
    theme(legend.position="bottom") +
    scale_y_continuous(labels = percent) +
    stat_smooth(method = "gam", 
                formula = y ~ s(x, k=10, sp=2, bs="ps"), 
                alpha=.3,
                level = .65)
```

The retrospective analysis allows us to surmise and compare to a "best case scenario", the most efficient order of restaurants to inspect based on their risk. In this case, we surmise the best case scenario is where every critical violation is found Below, a graph shows the difference between the most efficient path

```{r, fig.width=10, fig.height=8}
ggplot(melt(data = comp_summary_cumsum, 
            id.vars = "\nInspection Date")) +
    aes(x = `\nInspection Date`, 
        y = value, 
        colour = variable) + 
    labs(title=paste0("Comparing cumulative violations discovered\n",
                      "with additional optimal path\n")) +
    ylab("Cumulative critical violations to date\n") +
    theme(legend.position="bottom") +
    geom_line(lwd = 1.5)
```


# Summary
This model was able to reduce the timeframe to discover critical violations at Chicago's food establishments. The sanitarians proved to be significant predictors of finding critical violations, as did the outcomes of previous food inspections, the time since the last inspection, whether the establishment had an alcohol consumption and tobacco retail licenses, the average temperature, nearby burglaries and sanitation though 311. Older businesses and the number of garbage complaints through 311 were negatively related to finding critical violations.

Within a two-month window, the average time to find a critical violation was reduced by `r sprintf("%0.2f", mean(time_diff))` days, a statistically significant finding. 

At times, we've explicitly assumed that finding violations is time invariant throughout the pilot phase. That is, a food establishment found with a critical violation on day 40 would have also been found to have a violation even if it was inspected earlier. Unfortunately, we do not have a method to test this assumption. However, the relatively short time window of the study helped ensure external factors, such as severe weather, had limited impact on temporal violations.

While we found inspectors are helpful in fitting and explaining the variation of restaurants, it is not practical to include inspector estimates on a daily basis. Inspectors may be reassigned based on absences, uneven workload, and other factors. Thus, the daily model removes inspectors from generating the estimates levels for each restaurant. 

Likewise, weather is included, but effectively undifferentiating factor in daily estimates. Our weather model uses a single temperature across the entire city, which does not impact any region more than the other. Since we're only moving the intercept for all restaurants, the inclusion of weather only provides a more meaningful predicted value of a food establishment, but does not help change the order of inspections.

Additional data can also be used to supplement this model. Restaurant review data, such as the Google Places API or Yelp, could help supplement data on the conditions of a food establishment. Because this is an open-source model, outside researchers will be able to freely reproduce, verify, and suggest improvements to the model presented in this paper. Researchers can submit suggestions and modifications by creating a [pull reqest](https://help.github.com/articles/using-pull-requests/). The instructions to submitting a pull request can be found in this study's [contributing guidelines](https://github.com/Chicago/food-inspections-evaluation/blob/master/README.md).

```{r references, echo=FALSE, message=FALSE}
geneorama::set_project_dir("food-inspections-evaluation")
write.bibtex(file="REPORTS/references.bib")
```

# References
